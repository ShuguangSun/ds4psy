---
title: "Test Questions (ds4psy)"
author: "Hansjörg Neth, SPDS, uni.kn"
date: "2018 05 31"
output:
   rmdformats::html_clean: # html_clean html_docco readthedown material #
     code_folding: show # hide
     toc_float: true
     toc_depth: 2
     highlight: kate # textmate default kate haddock monochrome #
     lightbox: true # true by default
     fig_width: 8 # in inches
editor_options: 
  chunk_output_type: console # inline
---

<!-- Collection of exercises and test problems | ds4psy: Summer 2018 -->

```{r preamble, echo = FALSE, cache = FALSE, message = FALSE, warning = FALSE}
## (a) Housekeeping: -----
rm(list=ls()) # clean all.

## (b) Current file name and path: ----- 
# cur.path <- dirname(rstudioapi::getActiveDocumentContext()$path)
# cur.path
# setwd(cur.path) # set to current directory
setwd("~/Desktop/stuff/Dropbox/_code/R/_teachR/ds4psy/R/_testQuest") # set to current directory
# list.files() # all files + folders in current directory
fileName <- "tq_now.Rmd"

## (c) Packages: ----- 
library(knitr)
library(rmdformats)
library(tidyverse)

## (d) Global options: ----- 
options(max.print = "75")
opts_chunk$set(echo = TRUE,
	             cache = TRUE,
               prompt = FALSE,
               tidy = FALSE,
               collapse = TRUE, # set TRUE in answers 
               comment = "#>",
               message = FALSE,
               warning = FALSE,
               ## Default figure options:
               fig.width = 8, 
               fig.asp = .618, # golden ratio
               out.width = "75%",
               fig.align = "center"
               )
opts_knit$set(width = 75)

## (e) Graphics: ----- 

# Defining colors:
seeblau <- rgb(0, 169, 224, names = "seeblau", maxColorValue = 255) # seeblau.4 (non-transparent)

seeblau.colors <- c(rgb(204, 238, 249, maxColorValue = 255), # seeblau.1
                    rgb(166, 225, 244, maxColorValue = 255), # seeblau.2 
                    rgb(89, 199, 235, maxColorValue = 255),  # seeblau.3
                    rgb(0, 169, 224, maxColorValue = 255),   # seeblau.4 
                    rgb(0, 0, 0, maxColorValue = 255),       #  5. black
                    gray(level = 0, alpha = .6),             #  6. gray 60% transparent
                    gray(level = 0, alpha = .4),             #  7. gray 40% transparent
                    gray(level = 0, alpha = .2),             #  8. gray 20% transparent
                    gray(level = 0, alpha = .1),             #  9. gray 10% transparent
                    rgb(255, 255, 255, maxColorValue = 255)  # 10. white
                    )

unikn.pal = data.frame(                             ## in one df (for the yarrr package): 
  "seeblau1" = rgb(204, 238, 249, maxColorValue = 255), #  1. seeblau1 (non-transparent)
  "seeblau2" = rgb(166, 225, 244, maxColorValue = 255), #  2. seeblau2 (non-transparent)
  "seeblau3" = rgb( 89, 199, 235, maxColorValue = 255), #  3. seeblau3 (non-transparent)
  "seeblau4" = rgb(  0, 169, 224, maxColorValue = 255), #  4. seeblau4 (= seeblau base color)
  "black"    = rgb(  0,   0,   0, maxColorValue = 255), #  5. black
  "seegrau4" = rgb(102, 102, 102, maxColorValue = 255), #  6. grey40 (non-transparent)
  "seegrau3" = rgb(153, 153, 153, maxColorValue = 255), #  7. grey60 (non-transparent)
  "seegrau2" = rgb(204, 204, 204, maxColorValue = 255), #  8. grey80 (non-transparent)
  "seegrau1" = rgb(229, 229, 229, maxColorValue = 255), #  9. grey90 (non-transparent)
  "white"    = rgb(255, 255, 255, maxColorValue = 255), # 10. white
  stringsAsFactors = FALSE)

## (f) Exercise counter: ----- 
nr <- 0
```

```{r utility_add_random_NA_values, echo = FALSE, eval = TRUE}
# Adding a random amount (number or proportion) of NA or other values to a vector:

## Function to replace a random amount of vector elements by NA values:  
add_NAs <- function(vec, amount){
  
  stopifnot((is.vector(vec)) & (amount >= 0) & (amount <= length(vec)))

  out <- vec
  n <- length(vec)
  
  amount2 <- ifelse(amount < 1, round(n * amount, 0), amount) # turn amount prop into n
  
  out[sample(x = 1:n, size = amount2, replace = FALSE)] <- NA
  
  return(out)

}

## Check:
# add_NAs(1:10, 0)
# add_NAs(1:10, 3)
# add_NAs(1:10, .5)
# add_NAs(letters[1:10], 3)

## Generalization: Replace a random amount of vector elements by what: 
add_whats <- function(vec, amount, what = NA){
  
  stopifnot((is.vector(vec)) & (amount >= 0) & (amount <= length(vec)))

  out <- vec
  n <- length(vec)
  
  amount2 <- ifelse(amount < 1, round(n * amount, 0), amount) # turn amount prop into n
  
  out[sample(x = 1:n, size = amount2, replace = FALSE)] <- what
  
  return(out)

}

## Check:
# add_whats(1:10, 3) # default: what = NA
# add_whats(1:10, 3, what = 99)
# add_whats(1:10, .5, what = "ABC")

```

# Introduction

## Course Coordinates

<!-- uni.kn logo and link to SPDS: -->  
<!-- ![](./inst/pix/uniKn_logo.png) --> 
<a href="https://www.spds.uni-konstanz.de/">
<img src = "../../inst/pix/uniKn_logo.png" alt = "spds.uni.kn" align = "right" width = "300" style = "width: 300px; float: right; border:20;"/>
<!-- <img src = "./inst/pix/uniKn_logo_s.png" alt = "spds.uni.kn" style = "float: right; border:20;"/> --> 
</a>

* Taught at the [University of Konstanz](https://www.uni-konstanz.de/) by [Hansjörg Neth](http://neth.de/) (<h.neth@uni.kn>,  [SPDS](https://www.spds.uni-konstanz.de/), office D507).
* Spring/summer 2018: Mondays, 13:30--15:00, C511 (from 2018.04.16 to 2018.07.16) 
* Links to [ZeUS](https://zeus.uni-konstanz.de:443/hioserver/pages/startFlow.xhtml?_flowId=showEvent-flow&unitId=5101&termYear=2018&termTypeValueId=1&navigationPosition=hisinoneLehrorganisation,examEventOverviewOwn) and [Ilias](https://ilias.uni-konstanz.de/ilias/goto_ilias_uni_crs_758039.html)

## Test Questions

This file contains practice and test questions suited to test your skills and understanding. It also illustrates the procedure of our **mid-term exam** (on June 4, 2018).

## Preparation and response format

**`r nr`.** Please answer the following questions by creating a single R script (or an R-Markdown file `.Rmd`) that contains all your code and answers and meets the following criteria: 

- _Layout issues_: 

    1. Include a header that contains your _name_, _student ID_, this _course_, and today's _date_.
    
    2. Load the R packages of the `tidyverse`. 
    
    3. Structure your file clearly by _labeling_ the current task (e.g., `# Task 1: -----`) and subtask (e.g., `# (a) ...:`) and by _leaving blank lines_ between all tasks and subtasks. 
    
    Here's a layout template that you can copy and adapt: 

```{r layout_template, echo = TRUE}
## Mid-term exam  | Data science for psychologists (Summer 2018)
## Name: ... | Student ID: ...
## 2018 06 04
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##

## Preparations: ----- 

library(tidyverse)


## Task 1: ----- 

## (a) Saving and inspecting data:
sp <- as_tibble(sleep)
dim(sp)

## Answer: The sleep data contains 20 rows and 3 columns. 

## (b): ... 
## ...


## Task X: ----- 
## ...

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##
## End of file. ----- 
```

- Save your script (regularly) as `Lastname_Firstname_midTerm_180604.R` (replacing `Lastname` and `Firstname` by your names).

- When asked for numbers or interpretations, include _short answers as comments_ in your script. However, when asked for quantitative summaries containing more than 2 numbers (e.g., descriptive statistics of a dataset) simply print your results (e.g., the output of a `dplyr` pipe) in your code.

- Submit your script by email (as an attachment) to <h.neth@uni.kn> by 15:15 (today). 


# 3: Data visualisation

## Key skills

The chapter introduces visualizations (with `ggplot`), but not yet data transformations (with `dplyr`). 
Key skills to be acquired in this context include creating: 

- scatterplots (`geom_point`) and trendlines (`geom_line` and `geom_smooth`), 
- grouping (via aesthetic mappings like `color`, `shape`, `size`, etc.), 
- facetting (`facet_wrap` and `facet_grid`), 
- bar charts (`geom_bar`) for given values (`stat = "identity"`), frequency counts, or proportions, for different bar positions (`stack`, `fill`, and `dodge`),  
- boxplots (`geom_boxplot`), and 
- adjusting visual aspects (colors, shapes, themes), labels (e.g., plot titles and captions), and coordinates (`coord_cartesian`, `coord_flip`, and `coord_polar`). 

## Example tasks 

### Using `PlantGrowth` data

**`r nr<-nr+1; nr`.** The `PlanthGrowth` data (contained in R datasets) reports the results from an experiment that  compares growth yields (measured by the dried weight of plants) obtained under 2 treatments vs. a control condition.

**`r nr`a.** Save the `PlantGrowth` data as a tibble and inspect its dimensions (1 point).

**`r nr`b.** Use a `dplyr` pipe to compute the number of observations (rows) in each `group` 
and key descriptives (their mean, median, and standard deviation) (2).

**`r nr`c.** Use `ggplot` to create a graph that shows the medians and raw values of plant `weight` by `group` (2). Hint: Use 2 different geoms to show both medians and raw values in the same plot. The order of layers is determined by the order of geom commands. 

```{r plantGrowth, echo = TRUE, eval = TRUE, fig.show = "hold"}
# ?datasets::PlantGrowth

# (a) Save as tibble and inspect data:
pg <- as_tibble(PlantGrowth)
pg # => 30 cases (rows) x 2 variables (columns)

# (b) Compute number of observations by group, their mean, median and standard deviation: 
pg %>%
  group_by(group) %>%
  summarise(count = n(),
            mn_weight = mean(weight),
            md_weight = median(weight),
            sd_weight = sd(weight)
            )

# (c) Plot the median and raw values of weight by group: 
ggplot(pg, aes(x = group, y = weight)) +
  # geom_violin() +
  geom_boxplot(aes(fill = group)) +
  geom_point(aes(shape = group), alpha = 2/3, size = 4, position = "jitter") +
  labs(title = "Plant weight by group", x = "Group", y = "Weight", 
       caption = "[Using datasets::PlantGrowth data.]") +
  theme_bw()
```

### Using `sleep` data

**`r nr<-nr+1; nr`.** The `sleep` data (contained in R datasets) shows the effect of 2 sleep-promoting drugs (as an increase in hours of sleep compared to a control group) for 10 patients.

**`r nr`a.** Save the `sleep` data as a tibble and inspect its dimensions (1 point). 

**`r nr`b.** Use a `dplyr` pipe to compute the number of observations (rows) by group  
and key descriptives (their mean, median, and standard deviation) (2).

**`r nr`c.** Use `ggplot` to create a graph that shows the medians and raw values of `extra` sleep time by `group` (2). Hint: Use 2 different geoms to show both medians and raw values in the same plot. The order of layers is determined by the order of geom commands. 

**`r nr`d.** Reformat the `sleep` data so that the 2 groups appear in 2 lines (rows) and 10 subject IDs as 10 columns (1).

```{r, sleep, echo = TRUE, fig.show = "hold"}
# ?datasets::sleep

# (a) Save data as tibble and inspect:
sp <- as_tibble(sleep)
sp # => 20 cases (rows) x 3 variables (columns)

# (b) Compute the number of observations and descriptives of extra time by group:
sp %>% 
  group_by(group) %>%
  summarize(n = n(),
            md = median(extra),
            mn = mean(extra),
            sd = sd(extra))

# (c) Visualize the raw values and averages by group:
ggplot(sp, aes(x = group, y = extra, color = group)) +
  geom_boxplot() +
  # geom_violin() +
  geom_point(aes(shape = group), size = 4, position = "jitter") + 
  labs(title = "Extra sleep time by treatment group", x = "Treatment group", y = "Extra sleep time", 
       caption = "[Using datasets::sleep data.]") +
  theme_bw()

# (d) Reformat data so that the 2 groups appear in 2 lines, 
#     and 10 subject IDs as 10 columns:
sp %>%
  spread(key = ID, value = extra)
```

### Using `ChickWeight` data

**`r nr<-nr+1; nr`.** The `ChickWeight` data (contained in R datasets) contains the results of an experiment that measures the effects of `Diet` on the early growth of chicks.

**`r nr`a.** Save the `ChickWeight` data as a tibble and inspect its dimensions (1 point). 

**`r nr`b.** Create a line plot showing the `weight` development of each indivdual chick (on the y-axis) over `Time` (on the x-axis) for each `Diet` (in 4 different facets) (2).

```{r, chickWeight, echo = TRUE}
# ?datasets::ChickWeight

# (a) Save data as tibble and inspect:
cw <- as_tibble(ChickWeight)
cw # => 578 observations (rows) x 4 variables (columns)

# (b) Scatter and/or line plot showing the weight development of each chick (on the y-axis) 
#     over Time (on the x-axis) for each Diet (as different facets): 
ggplot(cw, aes(x = Time, y = weight, group = Diet)) +
  facet_wrap(~Diet) + 
  geom_point(alpha = 1/2) +
  geom_line(aes(group = Chick)) +
  geom_smooth(aes(color = Diet)) + 
  labs(title = "Chick weight by time for different diets", x = "Time (number of days)", y = "Weight (in gm)", 
       caption = "[Using datasets::ChickWeight data.]") +
  theme_bw()
```

**`r nr`c.** The following bar chart shows the number of chicks per `Diet` over `Time`.  
We see that the initial `Diet` groups contain a different numbers of chicks and some chicks drop out over `Time`.  

```{r, chickWeight_2, echo = TRUE}
# (c) Bar plot showing the number (count) of chicks per diet over time: 
ggplot(cw, aes(x = Time, fill = Diet)) +
  geom_bar(position = "dodge") +
  labs(title = "Number of chicks per diet over time", x = "Time (number of days)", y = "Number", 
       caption = "[Using datasets::ChickWeight data.]") +
  theme_bw()
```

Instead of re-creating this plot, create a table (or tibble) that shows the same (4 x 12 = 48) data points in 4 rows (for the 4 different types of `Diet`) and 12 columns (for the different `Time` points) (2).

Hint: First count the number of chicks per `Diet` and `Time`, then spread the results into a wider format (to show the time points as different columns).

```{r, chickWeight_3, echo = TRUE}
# (c) Re-create the counts of chicks per diet over time numerically (using `dplyr` and `tidyr`). 
#     as a table: How many chicks are there per diet over time?
cw %>%
  group_by(Diet, Time) %>%
  count() %>%
  spread(key = Time, value = n) 

## Not asked: 
# (x) Plot the weight of each individual chick 
# and the Median weight per diet at the end (Time = 21).
# Hint: Filter data for the maximum time and 
#       combine 2 geoms: A boxplot and a scatterplot.
cw %>%
  filter(Time == 21) %>%
  ggplot(., aes(x = Diet, y = weight, fill = Diet)) +
    geom_boxplot() +
    geom_point(size = 4, alpha = 1/2) +
    coord_flip() +
    theme_bw()
```

### Using `tidyr::table1` data 

**`r nr<-nr+1; nr`.** The `tidyr::table1` shows the number of TB cases for 3 countries and 2 years. 

**`r nr`a.** Plot a bar chart that shows the number of cases per `country` (on the y-axis) as a function of the `year` (on the x-axis) (2).

**`r nr`b.** Format the bars (showing cases per `country`) in different colors (1). 

**`r nr`c.** Provide a suitable plot title and a caption noting the data source (1). 

**`r nr`d.** Label each bar with the number of cases (1). 

```{r, barchart_with_labels, echo = FALSE, eval = TRUE}
# ?geom_bar
?tidyr::table1

ggplot(tidyr::table1, aes(x = year, y = cases, fill = country)) + 
  geom_bar(stat = "identity", position = "dodge", color = "black") + 
  geom_text(aes(label = cases), position = position_dodge(width = 0.9), vjust = -0.5) +
  scale_x_continuous(name = "Year", breaks = 1999:2000) + 
  labs(title = "Cases per country and year", y = "Cases", caption = "[Using tidyr::table1 data.]") +
  theme_classic()
```

### Own data: German election results

**`r nr<-nr+1; nr`.** The following table provides the percentage share of 2 major parties on the last 2 
general elections of Germany (based on [this link](https://www.bundeswahlleiter.de/info/presse/mitteilungen/bundestagswahl-2017/34_17_endgueltiges_ergebnis.html)):

| Party:  | Share 2013:       | Share 2017:       |
|:------- |--------:|--------:|
| CDU/CSU | `r (34.1 + 7.4)`% | `r (26.8 + 6.2)`% |
| SPD     | `r (25.7)`%       | `r (20.5)`%       |
| Others  |      `?`          |    `?`            | 

<!-- Details: from 
https://www.bundeswahlleiter.de/info/presse/mitteilungen/bundestagswahl-2017/34_17_endgueltiges_ergebnis.html 

CDU 	Christlich Demokratische Union Deutschlands 	26,8 % 	(2013: 34,1 %)
SPD 	Sozialdemokratische Partei Deutschlands 	    20,5 % 	(2013: 25,7 %)
AfD 	Alternative für Deutschland 	                12,6 % 	(2013:  4,7 %)
FDP 	Freie Demokratische Partei 	                  10,7 % 	(2013:  4,8 %)
DIE LINKE 	DIE LINKE 	                             9,2 % 	(2013:  8,6 %)
GRÜNE 	    BÜNDNIS 90/GRÜNE 	                       8,9 % 	(2013:  8,4 %)
CSU 	Christlich-Soziale Union in Bayern e.V 	       6,2 % 	(2013:  7,4 %)
Sonstige 	  	                                       5,0 % 	(2013:  6,2 %)

--> 

**`r nr`a.** Create a tibble `de` that contains this data and the missing (`?`) values for all other parties so that all shares of an election add up to 100% (2). 

**`r nr`b.** Convert your `de` table into a "tidy" table saved as `de_2` (2). 

Hints: Use `gather` to list the values of all election results in 1 variable called `share` and make sure that `de_2` contains a separate variable (column) that specifies the election `year`.

**`r nr`c.** Visualize and contrast the election results by a bar chart that contains 2 bars (representing the 2 elections) and the party's share of votes (as the proportions of each bar) (2). 

Hints: As the data in `de_2` already contains the identity of the values which you want to plot, there is no need to count anything. Showing multiple values in one bar is called a "stack".

**`r nr`d.** Create an improved version of your plot in **`r nr`c.** that provides the share of votes for each bar area as text labels (1).

**`r nr`e.** Show the 2017 election results as a pie chart (with 3 pieces representing each party's share of votes) (1).

```{r election_results}
## (a) Create a tibble with the data:
de <- tibble(
    party = c("CDU/CSU", "SPD", "Others"),
    share_2013 = c((.341 + .074), .257, (1 - (.341 + .074) - .257)), 
    share_2017 = c((.268 + .062), .205, (1 - (.268 + .062) - .205))
  )
de$party <- factor(de$party, levels = c("CDU/CSU", "SPD", "Others"))  # optional
de

## Check that columns add to 100:
sum(de$share_2013)  # => 1 (qed)
sum(de$share_2017)  # => 1 (qed)

## (b) Converting de into a tidy data table:
de_2 <- de %>%
  gather(share_2013:share_2017, key = "election", value = "share") %>%
  separate(col = "election", into = c("dummy", "year")) %>%
  select(year, party, share)
de_2

## Note that year is of type character, which could be changed by:
# de_2$year <- parse_integer(de_2$year)

## (c) Bar chart showing proportions for each election:
ggplot(de_2, aes(x = year, y = share, fill = party)) +
  ## (A) 1 bar per election (position = "stack"):
  geom_bar(stat = "identity", position = "stack") +  # 1 bar per election
  ## (B) 3 bars per election (position = "dodge"):  
  # geom_bar(stat = "identity", position = "dodge") +  # 3 bars next to each other
  ## Pimping plot: 
  scale_fill_manual(values = c("black", "red3", "gold")) + # optional
  labs(title = "Partial results of the German general elections 2013 and 2017", 
       x = "Year of election", y = "Share of votes", caption = "[Data: www.bundeswahlleiter.de.]") +
  theme_classic()

## Advanced issues or Bonus tasks: ----- 

cols <- rep(c("black", "black", "white"), 2)

## (d) Versions of (c) with vote shares as text labels:
ggplot(de_2, aes(x = year, y = share, fill = party, 
                 label = paste0(as.character(round(share * 100, 2)), "%"))) +
  ## (A) 1 bar per election (position = "stack"):
  geom_bar(stat = "identity") +  # 1 bar per election
  geom_text(position = "stack", vjust = 5, size = 4, color = rep(c("black", "black", "white"), 2)) + 
  ## (B) 3 bars per election (position = "dodge"):  
  # geom_bar(stat = "identity", position = "dodge") +  # 3 bars next to each other
  # geom_text(position = position_dodge(width = .9), vjust = -1, size = 4) + 
  ## Pimping plot: 
  scale_fill_manual(values = c("black", "red3", "gold")) + # optional
  labs(title = "Partial results of the German general elections 2013 and 2017", 
       x = "Year of election", y = "Share of votes", caption = "[Data: www.bundeswahlleiter.de.]") +
  theme_classic()

## (e) Pie chart of 2017 results only:
ggplot(de, aes(x = "", y = share_2017)) +
  geom_bar(aes(fill = party), stat = "identity", position = "stack") +  # 1 bar per election
  scale_fill_manual(values = c("black", "red3", "gold")) +  # optional
  ## Pimping plot: 
  labs(title = "Partial pie of the German general election 2017", 
       y = "Share of votes", caption = "[Data: www.bundeswahlleiter.de.]") +
  coord_polar("y") +
  theme_minimal()

## OLDER code (based on 2017 data ONLY): ----- 

## (x) Create a data frame:
df <- data.frame(
    party = c("CDU/CSU", "SPD", "Others"),
    # share_2013 = c((.341 + .074), .257, (1 - (.341 + .074) - .257)), 
    share_2017 = c((.268 + .062), .205, (1 - (.268 + .062) - .205))
  )
df$party <- factor(df$party, levels = c("CDU/CSU", "SPD", "Others"))
df

## (y) Create a stacked bar chart:
bp <- ggplot(data = df, mapping = aes(x = "", y = share_2017, fill = party)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = c("black", "red3", "gold")) + 
  theme_bw()
bp

## (z) Create a pie chart: 
pie <- bp + 
  coord_polar("y", start = 0) +
  theme_minimal()
pie
```

- Create a data frame or tibble
- Plot bar chart (with `stat = "identity"`)
- Create pie chart (with `coord_polar`) 

# 5: Data transformation

```{r, setup_5, echo = TRUE, eval = TRUE}
library(tidyverse)    # dplyr
library(nycflights13) # data
```

## Key skills

This chapter illustrates the basic table-manipulation tools (verbs) of `dplyr`. 
Key skills conveyed include transforming data by using essential `dplyr` commands:

- `filter` and `arrange` cases (rows); 
- `select` and re-arranging variables (columns); 
- computing and adding new variables (with `mutate` and `transmute`); 
- computing counts and descriptives of group aggregates (by `group_by`, `summarise`, and functions); 
- computing new group-level variables (by grouped mutates). 

## Example tasks 

### Using `dplyr::starwars` dataset

**`r nr<-nr+1; nr`.** Let's tackle the universe with the tidyverse by uncovering some facts about the `dplyr::starwars` dataset. 

Answer the following questions by using pipes of basic `dplyr` commands (i.e., arranging, filtering, selecting, grouping, counting, summarizing).

**`r nr`a.** Save the tibble `dplyr::starwars` as `sw` and reports its dimensions (1). 

**`r nr`b.** Missing values and known unknowns:

- How many missing (`NA`) values does `sw` contain? (1)  

- Which individuals come from an unknown (missing) `homeworld` but have a known `birth_year` or known `mass`? (1)

<!-- Which variable (column) has the most missing values? --> 

<!-- Replace all missing values of `hair_color` (in the variable `sw$hair_color`) by "bald" (2). -->


**`r nr`c.** Gender issues:

- How many humans are contained in `sw` overall and by gender? (1)

- How many and which individuals in `sw` are neither male nor female? (1)

- Of which species in `sw` exist at least 2 different gender values? (1)


**`r nr`d.** Popular homes and heights:

- From which `homeworld` do the most indidividuals (rows) come from? (1)

- What is the mean `height` of all individuals with orange eyes from the most popular homeworld? (1)


**`r nr`e.** Seize and mass issues: 

- Compute the median, mean, and standard deviation of `height` for all droids (1).

- Compute the average height and mass by species and save the result as `h_m` (1).

- Sort `h_m` to list the 3 species with the smallest individuals (in terms of mean height) (1).

- Sort `h_m` to list the 3 species with the heaviest individuals (in terms of median mass) (1).

```{r, starwars_transformations}
# library(tidyverse)
# ?dplyr::starwars

## (a) Basic data properties: ---- 
sw <- dplyr::starwars
dim(sw)  # => 87 rows (denoting individuals) x 13 columns (variables) 

## Missing data: ----- 

## (+) How many missing data points?
sum(is.na(sw))  # => 101 missing values.

# (+) Which individuals come from an unknown (missing) homeworld 
#     but have a known birth_year or mass? 
sw %>% 
  filter(is.na(homeworld), !is.na(mass) | !is.na(birth_year))


## (x) Which variable (column) has the most missing values?
colSums(is.na(sw))  # => birth_year has 44 missing values
colMeans(is.na(sw)) #    (amounting to 50.1% of all cases). 

## (x) Replace all missing values of `hair_color` (in the variable `sw$hair_color`) by "bald": 
# sw$hair_color[is.na(sw$hair_color)] <- "bald"


## (c) Gender issues: ----- 

# (+) How many humans are there of each gender?
sw %>% 
  filter(species == "Human") %>%
  group_by(gender) %>%
  count()

## Answer: 35 Humans in total: 9 females, 26 male.

# (+) How many and which individuals are neither male nor female?
sw %>% 
  filter(gender != "male", gender != "female")

# (+) Of which species are there at least 2 different gender values?
sw %>%
  group_by(species, gender) %>%
  count() %>%  # table shows species by gender: 
  group_by(species) %>%  # Which species appear more than once in this table? 
  count() %>%
  filter(nn > 1)

## (d) Homeworld issues: ----- 

# (+) Popular homes: From which homeworld do the most indidividuals (rows) come from? 
sw %>%
  group_by(homeworld) %>%
  count() %>%
  arrange(desc(n))
# => Naboo (with 11 individuals)

# (+) What is the mean height of all individuals with orange eyes from the most popular homeworld? 
sw %>% 
  filter(homeworld == "Naboo", eye_color == "orange") %>%
  summarise(n = n(),
            mn_height = mean(height))

## Note: 
sw %>% filter(eye_color == "orange") # => 8 individuals


# (+) What is the mass and homeworld of the smallest droid?
sw %>% 
  filter(species == "Droid") %>%
  arrange(height)

## (4) Group summaries: ----- 

# (+) Compute the median, mean, and standard deviation of `height` for all droids.
sw %>%
  filter(species == "Droid") %>%
  summarise(n = n(),
            not_NA_h = sum(!is.na(height)),
            md_height = median(height, na.rm = TRUE),
            mn_height = mean(height, na.rm = TRUE),
            sd_height = sd(height, na.rm = TRUE))

# (+) Compute the average height and mass by species and save the result as `h_m`:
h_m <- sw %>%
  group_by(species) %>%
  summarise(n = n(),
            not_NA_h = sum(!is.na(height)),
            mn_height = mean(height, na.rm = TRUE),
            not_NA_m = sum(!is.na(mass)),
            md_mass = median(mass, na.rm = TRUE)
            )
h_m

# (+) Use `h_m` to list the 3 species with the smallest individuals (in terms of mean height)?
h_m %>% arrange(mn_height) %>% slice(1:3)

# (+) Use `h_m` to list the 3 species with the heaviest individuals (in terms of median mass)?
h_m %>% arrange(desc(md_mass)) %>%  slice(1:3)


## (+) Other questions: ----- 

# (+) How many individuals come from the 3 most frequent (known) species?
sw %>%
  group_by(species) %>%
  count %>%
  arrange(desc(n)) %>%
  filter(n > 1)
```

### Using `nycflights13::weather` dataset

Use the data set `nycflights13::weather` for questions that require 
`filter`, `arrange`, `select`, `group_by`, `summarise` (count, NAs, means, medians), etc.

```{r, weather_transformations}
library(nycflights13)
nycflights13::weather
# ?weather

## How many observations (rows) and variables (columns) does the data set contain overall?
dim(weather)

## How many missing values does the `weather` data contain?
sum(is.na(weather))  # 
mean(is.na(weather)) # percentage

## What is the range of values of the `year` variable?
range(weather$year)

## How many observations (rows) does the data contain for each of the 3 airports (`origin`)?
weather %>%
  group_by(origin) %>%
  count()

## (On the original data set): 
## Compute a variable `temp_dc` that provides the temperature (in degrees Celsius) 
## that corresponds to `temp` (in degrees Fahrenheit).
## Fahrenheit (degrees F) to Celsius (degrees C) conversion:
## C = (F - 32) x 5/9.

## Add your new `temp_dc` variable to a new dataset `weather_2` and 
## re-arrange its columns so that your `temp_dc` variable appears next to `temp`.

weather_2 <- weather %>%
  mutate(temp_dc = (temp - 32) * 5/9) %>%
  select(origin:temp, temp_dc, everything())
weather_2

## Only considering "JFK" airport: 
## What are the 3 (different) dates with the (a) coldest and (b) hottest temperatures there?
## Report the 3 dates and their extreme temperatures (in degrees Celsius) for (a) and (b). 
JFK_temp <- weather_2 %>%
  filter(origin == "JFK") %>%
  arrange(temp_dc)

JFK_temp # => coldest days
JFK_temp %>% arrange(desc(temp)) # => hottest days


## Aggregation examples: -----

## (1) Average temperature per month: 
##     (used in class): 

weather %>%
  # group_by(origin, month) %>%
  group_by(month) %>%
  summarise(n = n(),
            n_not_NA = sum(!is.na(temp)), 
            mn_temp = mean(temp, na.rm = TRUE)) %>%
  ggplot(aes(x = month, y = mn_temp)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:12) +
  theme_bw()

# (2) Average precipitation (by origin and month):

weather %>%
  group_by(origin, month) %>%
  summarise(n = n(),
            n_not_NA = sum(!is.na(precip)), 
            mn_precip = mean(precip, na.rm = TRUE)) %>%
  ggplot(aes(x = month, y = mn_precip, color = origin)) +
  geom_point() +
  geom_line() +
  # geom_bar(aes(fill = origin), stat = "identity", position = "dodge") +
  scale_x_continuous(breaks = 1:12) +
  theme_bw()

## Computation and visualization to answer a question: ----

## For each of the 3 airports: 
## When excluding extreme cases of precipitation (values of `precip` greater than 0.30): 
## Does it rain more during winter (Oct to Mar) or during summer (Apr to Sep) months?

## Inspect data:
weather # month is given numerically!

# ggplot(weather, aes(x = precip)) +
#  geom_histogram(binwidth = 0.01, fill = seeblau)

## (a) Preparation: Filter out extreme values and add a summer variable: 
weather_season <- weather %>%
  filter(precip < .30) %>%
  mutate(summer = (month > 3 & month < 10)#,
         # winter = (month < 4 | month > 9)
         )

## (b) Computation: Sum of precipitation by origin and summer season:
weather_season %>%
  group_by(origin, summer) %>%
  summarise(n = n(),
            # n_not_NA = sum(!is.na(precip)), 
            # mn_precip = mean(precip, na.rm = TRUE),
            sum_precip = sum(precip, na.rm = TRUE))

## (c) Visualization: Sum of precipitation as a bar chart:
ggplot(weather_season, aes(x = summer, fill = summer)) +
  facet_wrap(~origin) + 
  geom_bar(aes(weight = precip), na.rm = TRUE) +
  scale_fill_manual(values = c("steelblue3", "firebrick"))
```

### Own data: Identifying outliers

**`r nr<-nr+1; nr`.** An _outlier_ can be defined as someone whose value in some metric deviates by more than some criterion (e.g., 2 standard deviations) from the mean. But this definition is incomplete, as it does not specify the appropriate reference group. This task explores the implications of different reference groups.

```{r, outlier_create_data, echo = FALSE, eval = TRUE}
# library(tidyverse)

## Creating a suitable data set: 
set.seed(123)
n <- 1000
id <- paste0("nr.", 1:n) # paste0(sample(LETTERS, 1), sample(LETTERS, 1))
sex <- sample(x = c(0, 1), size = n, replace = TRUE)
height <- rep(NA, n)
noise_0 <- round(rnorm(n, mean = 0, sd = 8), 0)
noise_1 <- round(rnorm(n, mean = 0, sd = 11), 0)
height[sex == 0] <- 169 + noise_0[sex == 0]
height[sex == 1] <- 181 + noise_1[sex == 1]

## Modify data:
height <- add_NAs(height, amount = 18)  # 1.8% NA values in height
height[sex == 0] <- add_whats(vec = height[sex == 0], amount = 1, what = 202) # add a tall woman
# sex <- add_NAs(sex, amount = 3)          # 2  NA values in sex

## Save data as tibble: 
data <- as_tibble(data_frame(id, sex, height))
data$sex <- factor(data$sex, labels = c("female", "male"))
names(data) <- c("id", "sex", "height")

## Check data:
mean(data$sex == "female", na.rm = TRUE)  # => .507
mean(data$height, na.rm = TRUE)           # => 174.7006 (with seed 123)
```

**`r nr`a.** Save the data into a tibble `data` and report its number of observations and variables (1).  

**`r nr`b.** How many missing data values are there in `data`(1)? 

**`r nr`c.** What is the gender distribution in this sample (1)? 

**`r nr`d.** Create a graph that shows the distribution of `height` values for each gender (1).

<!-- Definition: Outlier -->

**`r nr`e.** Compute 2 new variables that signal and distinguish between 2 types of outliers in terms of `height`: 

1. outliers relative to the `height` of the _overall sample_ (i.e., people with `height` values deviating more than 2 SD from the overall mean of `height`) (1);

2. outliers relative to the `height` of _some subgroup's_ mean and SD. In the present case, the subgroup to consider is every person's gender (i.e., people with `height` values deviating more than 2 SD from the mean `height` of their own gender) (1).

Hint: Both variables can be defined as logical variables and added as new columns of `data` (via `mutate`). While the first variable can directly be computed based on the mean and SD of the overall sample, the second variable can be computed after grouping `data` by gender and then computing the corresponding mean and SD. 

**`r nr`f.** Use the 2 new variables to define and identify 2 subgroups of people: 

1. `out_1`: Individuals (females and males) with `height` values that are outliers relative to _both_ the entire sample _and_ the sample of their own gender. How many such individuals are in `data` (1)? 

2. `out_2`: Individuals (females and males) with `height` values that are _not_ outliers relative to the entire population, but _are_ outliers relative to their own gender. How many such individuals are in `data` (1)? 

**`r nr`g.** Visualize the raw values and distributions of `height` for both types of outliers (`out_1` and `out_2`) in separate plots and describe the key features of individuals shown in each plot (2). 



+++ here now +++

#### Tasks

A. Create 2 variables for both types of outliers.

B. Identify 

C. Identify people (men and women) who are _not_ outliers relative to the entire population, but _are_ outliers relative to their own sex. (As men are taller than women on average, these are tall women and small men.)

D. Visualize the overall distribution of `height`, its distribution by `sex`, 
and the raw data values for both types of outliers (by `sex`).

#### Solution 

To identify both groups, we first need to compute 2 outlier variables: One for the entire population and one for each subgroup (based on people's `sex`).

<!-- fig.show options: "asis", "hide", "hold", "animate" --> 

```{r, outlier_solution, fig.show = "asis"}
## (a) Save and inspect data:
data <- as_tibble(data)
dim(data)  # => 1000 observations (rows) x 3 variables (columns)

## (b) Missing data points: 
sum(is.na(data))  # => 18 missing values

## (c) Gender distribution: 
data %>% 
  group_by(sex) %>% 
  count()
# => 50.7% females, 49.3% males.

## (d) Distributions of `height` as density plot: 
ggplot(data, aes(x = height)) +
  geom_density(fill = "gold", alpha = 2/3) +
  geom_density(aes(fill = sex), alpha = 2/5) +
  labs(title = "Distribution of heights overall and by gender", 
       fill = "Gender") +
  theme_bw()

## Alternative solution as 2 histograms: 
ggplot(data) +
  facet_wrap(~sex) + 
  geom_histogram(aes(x = height, fill = sex), binwidth = 5, color = "grey10") +
  labs(title = "Distribution of heights overall and by gender",
       fill = sex) +
  scale_fill_manual(values = c("firebrick", "steelblue3")) +
  theme_bw()

## (+) Compute the number, means and SD of height values:
{
  ## 1. overall: 
  data %>%
    summarise(n = n(),
              n_not_NA = sum(!is.na(height)),
              mn_height = mean(height, na.rm = TRUE),
              sd_height = sd(height, na.rm = TRUE))
  
  ## 2. by gender:
  data %>%
    group_by(sex) %>%
    summarise(n = n(),
              n_not_NA = sum(!is.na(height)),
              mn_height = mean(height, na.rm = TRUE),
              sd_height = sd(height, na.rm = TRUE))
  }

## (e) Compute means, SDs, and outliers of overall and by sex: -----  
crit <- 2 # criterion for detecting outliers (in SD units)

data_out <- data %>%
  mutate(mn_height = mean(height, na.rm = TRUE),
         sd_height = sd(height, na.rm = TRUE),
         out_height = abs(height - mn_height) > (crit * sd_height)) %>%
  group_by(sex) %>%
  mutate(mn_sex_height = mean(height, na.rm = TRUE),
         sd_sex_height = sd(height, na.rm = TRUE),
         out_sex_height = abs(height - mn_sex_height) > (crit * sd_sex_height))
# data_out

## (f) Identify 2 types of outliers: 
## 1. Outliers relative to entire population AND to their own gender: 
out_1 <- data_out %>%
  filter(out_height & out_sex_height) %>%
  arrange(sex, height)

nrow(out_1) # => 21 individuals. 

## 2. Outliers relative to their own gender, but NOT relative to entire population:
out_2 <- data_out %>%
  filter(!out_height & out_sex_height) %>%
  arrange(sex, height)  

nrow(out_2) # => 24 individuals.

## Visualization of both outlier types (out_1 and out_2): ----- 

## (g) 2 types of outliers:
## 1. out_1: 
ggplot(out_1, aes(x = sex, y = height)) +
  geom_violin(aes(fill = sex)) + 
  geom_jitter(size = 4, alpha = 2/3) + 
  scale_fill_manual(values = c("firebrick", "steelblue3")) +
  labs(title = "Outliers relative to both overall sample and gender") +
  theme_bw()

# Interpretation: 
# `out_1` mostly contains short women (except for 1 tall woman) 
# and tall men (except for 2 short men). 

## 2. out_2: 
ggplot(out_2, aes(x = sex, y = height)) +
  geom_violin(aes(fill = sex)) + 
  geom_jitter(size = 4, alpha = 2/3) + 
  scale_fill_manual(values = c("firebrick", "steelblue3")) +
  labs(title = "Outliers relative to gender but not overall sample") +
  theme_bw()

# Interpretation: 
# `out_2` contains individuals which are either tall women or short men.
```

### Using `flight` dataset to compute duration

Using the `flights` dataset: 
- Compute a variable `true_duration` as the duration of each flight (in minutes) from its `dep_time` and `arr_time`. 
- How does it relate to the `air_time` variable in the data set? (Plot the relationship between both variables.)

```{r, flights_duration, echo = TRUE}
compute_duration <- function(dep_min, arr_min) {

  dur <- NA # initialize
  
  # Distinguish between 2 cases:
  if (dep_min < arr_min) {  # dep before arr (i.e., same day): 
    dur <- arr_min - dep_min
  } else { # dep later than arr (i.e., different days): 
    dur <- (arr_min + 24 * 60) - dep_min 
  }
  
  return(dur)
  
}

# Check for vectors: 
dep <- seq(0, 24*60, by = +15)
arr <- seq(24*60, 0, by = -15)

# compute_duration(dep, arr)
# ???: How to apply a function to each pair of values of 2 vectors/columns?

d <- flights %>% 
  filter(origin == "JFK") %>%  # to reduce size of dataset
  select(dep_time, arr_time, air_time) %>%
  mutate(dep_time_min = ((dep_time %/% 100) * 60) + (dep_time %% 100),
         arr_time_min = ((arr_time %/% 100) * 60) + (arr_time %% 100),
         # true_duration = compute_duration(dep_time_min, arr_time_min),
         true_duration = arr_time_min - dep_time_min) %>%
  filter(air_time > 0, true_duration > 0)

p <- ggplot(d, (aes(x = air_time, y = true_duration))) +
  geom_point(alpha = 1/4) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, size = 1, color = "red3") +
  theme_bw()
```

#### Notes

- The `dep_time` and `arr_time` are specified in terms of hour and minutes. As an hour contains 60 minutes (rather than 100), we first use modular arithmetic to transform both variables into a metric of minutes (see variables `dep_time_min` and `arr_time_min`).

- For flights departing and arriving on the same day, we can simply subtract `dep_time_min` from `arr_time_min` to obtain `true_duration`. However, if a flight departs before and arrives after midnight (i.e., arrives on the next day), this measure would yield a negative result. To correct for this, we need to add 24 hours (24 times 60 minutes) to `arr_time` whenever `arr_time` is before (or smaller than) `dep_time`. [This assumes that there are no flights exceeding 24 hours.]

- Seems too difficult at this stage, as it requires _conditional execution_ (for flights departing and arriving on the same vs. different days) and/or _functions_. 

In chapter 16 (Dates and times: http://r4ds.had.co.nz/dates-and-times.html), this problem is solved by introducing a Boolean variable for `overnight` flights and re-computing `air_time` by subtracting date-time objects (as intervals). 


# 7: Exploratory Data Analysis 

## Key skills

Transform and visualize datasets to: 

- detect and deal with missing values; 
- view distributions of variables and outliers;   
- plot and interpret relationships between (categorical/continuous) variables. 

## Example tasks 

Examples of tasks are contained in the other chapters (see tasks on "multiple chapters" below).

### Using `mtcars` data

**`r nr<-nr+1; nr`.** The `mtcars` data (contained in R datasets) contains ...

```{r, mtcars, echo = TRUE}
## Data to use:
# ?datasets::mtcars
# mtcars

## (0) Convert into a tibble: 
df <- as_tibble(rownames_to_column(mtcars, var = "model"))
df$cyl <- factor(df$cyl)
df$am <- factor(df$am)
df

## (1) Distribution of mpg: ---- 
range(df$mpg)

ggplot(df, aes(x = mpg)) +
  geom_histogram(binwidth = 5) 

## (2) Group means and boxplot: Mean mpg by cylinder: ----  

df %>%
  group_by(cyl) %>%
  summarise(n = n(),
            md_mpg = median(mpg),
            mn_mpg = mean(mpg)
            )

ggplot(df, aes(x = cyl, y = mpg, color = cyl)) +
  geom_boxplot() +
  # geom_violin() + 
  geom_jitter()

## (3) Scatterplot: ---- 

## Show value of `disp` (on y-axis) by `hp` (on x-axis), grouped by `am`: 
ggplot(df, aes(x = hp, y = disp, fill = am)) +
  geom_point(shape = 21, size = 2.5) +
  # geom_smooth() +
  geom_text(aes(label = model), size = 2.5, vjust = 0, hjust = 0, nudge_x = 5) +
  theme_bw()

## Alternative (using facets):
ggplot(df, aes(x = hp, y = disp)) +
  facet_wrap(~am) + 
  geom_point(size = 2.5) +
  # geom_smooth() +
  geom_text(aes(label = model), size = 2.5, vjust = 0, hjust = 0, nudge_x = 5) +
  theme_bw()

## Conclusions: ---- 
# - There is a positive correlation between horsepower and displacement 
#   (for both types of transmission). 
# - outliers: Ford Pantera L and Maserati 
#             have more hp and disp than other cars with manual transmission.
```

#### Tasks involved

- Plot variable distributions (histogram) 
- Descriptive group measures (count, mean, SD) 
- Plot raw data and group means (boxplot) 
- Plot scatterplot of 2 continuous variables  
- Interpret and draw conclusions 


# 10: Tibbles

## Key skills

Turn data into tibbles: 

- convert data frames into tibbles (by `as_tibble`) 
- create tibbles from tabular data (by `tibble` and `tribble`)

## Example tasks 

The commands to create and convert to tibbles are included in tasks to other chapters. 


# 12: Tidy Data

## Key skills

Wrangle data to: 

- identify and create tidy datasets
- `gather` (wider) datasets into longer ones
- `spread` (longer) datasets into wider ones
- `separate` and `unite` variables (columns)

## Example tasks 

### Using own stock (`st`) dataset

```{r stock_data_definition, echo = FALSE, eval = TRUE}
# Define the stock data (to be shown below) as a tibble: 
st <- tribble(
  ~stock, ~d1_start, ~d1_end, ~d2_start, ~d2_end, ~d3_start, ~d3_end,  
  #-----|----------|--------|----------|--------|----------|--------|
  "Amada",   2.5,     3.6,    3.5,       4.2,      4.4,       2.8,            
  "Betix",   3.3,     2.9,    3.0,       2.1,      2.3,       2.5,  
  "Cevis",   4.2,     4.8,    4.6,       3.1,      3.2,       3.7     
)
```


**`r nr<-nr+1; nr`.** The following table shows the start and end price of 3 stocks on 3 days (d1, d2, d3):

```{r stock_data_table, echo = FALSE, eval = TRUE}
knitr::kable(st, caption = "Stock data example showing the start and end prices of the shares of 3 companies on 3 days.")
```

<!-- Data as Rmd table in text: 
| stock  | d1_start | d1_end | d2_start | d2_end | d3_start | d3_end |  
|---------|---------|--------|----------|--------|----------|--------|
| "Amada" |   2.5  |   3.6   |   3.5    |  4.2   |   4.4    |   2.8  |            
| "Betix" |   3.3  |   2.9   |   3.0    |  2.1   |   2.3    |   2.5  |  
| "Cevis" |   4.2  |   4.8   |   4.6    |  3.1   |   3.2    |   3.7  |
--> 

**`r nr`a.** Create a tibble `st` that contains this data in this (wide) format (1). 

**`r nr`b.** Transform `st` into a longer table `st_long` that contains 18 rows and only 1 numeric variable for all stock prices (1). 
Adjust it so that the `day` and `time` appear as 2 separate columns (1). 

**`r nr`c.** Create a graph that shows the 3 stocks' `end` prices (on the y-axis) over the 3 days (on the x-axis) (1).

**`r nr`d.** Spread  `st_long` into a wider table that contains `start` and `end` prices as 2 distinct variables (columns) for each stock and day (1).

```{r, stock_data, echo = TRUE}
# library(tidyverse)

## (a) Enter stock data (in wide format) as a tibble:
st <- tribble(
  ~stock, ~d1_start, ~d1_end, ~d2_start, ~d2_end, ~d3_start, ~d3_end,  
  #-----|----------|--------|----------|--------|----------|--------|
  "Amada",   2.5,     3.6,    3.5,       4.2,      4.4,       2.8,            
  "Betix",   3.3,     2.9,    3.0,       2.1,      2.3,       2.5,  
  "Cevis",   4.2,     4.8,    4.6,       3.1,      3.2,       3.7     
)
dim(st)

## Note data structure: 
## 2 nested factors: day (1 to 3), type (start or end).

## (b) Change from wide to long format 
##     that contains the day (d1, d2, d3) and type (start vs. end) as separate columns:
st_long <- st %>%
  gather(d1_start:d3_end, key = "key", value = "val") %>%
  separate(key, into = c("day", "time")) %>%
  arrange(stock, day, time) # optional: arrange rows
st_long

## (c) Plot the end values (on the y-axis) of the 3 stocks over 3 days (x-axis):
st_long %>% 
  filter(time == "end") %>%
  ggplot(aes(x = day, y = val, color = stock, shape = stock)) +
  geom_point(size = 3) + 
  geom_line(aes(group = stock))

## (d) Change st_long into a wider format that lists start and end as 2 distinct variables (columns):
st_long %>%
  spread(key = time, value = val) %>%
  mutate(day_nr = parse_integer(str_sub(day, 2, 2))) # optional: get day_nr as integer variable

## (e) Note: Assume that stock data contains duplicate rows:
st2 <- rbind(st, st)
st2

## Gathering from wide to long format works as before: 
st_long2 <- st2 %>%
  gather(d1_start:d3_end, key = "key", value = "val") %>%
  separate(key, into = c("day", "time")) %>%
  arrange(stock, day, time) # optional: arrange rows
st_long2

## However, spreading from long to wider format yields error of "duplicate identifiers": 

# st_long2 %>%
#  spread(key = time, value = val) 

## would yield ERROR!

## Possible fix:
## (e1) Add an id variable that distinguishes between duplicate cases: 
st_long3 <- st_long2 %>%
  mutate(id = rep(1:2, nrow(st_long2)/2)) %>%  # adding a vector
  select(stock, id, everything())               # optional: re-arrange variables (columns)
# st_long3

## (e2) Now spreading from long to wider format succeeds: 
st_long3 %>%
  spread(key = time, value = val)
```


# Multiple chapters

## Using `iris` dataset

**`r nr<-nr+1; nr`.** The `iris` data (contained in R datasets) provides the measurements (in cm) of plant parts (length and width of _sepal_ and _petal_ parts) for 50 flowers from each of 3 species of iris (called _setosa_, _versicolor_, and _virginica_).

### Exploratory data analysis (EDA)

**`r nr`a.** Save `datasets::iris` a tibble `ir` that contains this data and inspect it. 
Are there any missing values? (2). 

**`r nr`b.** Compute a summary table that shows the means of the 4 measurement columns (`Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`) for each of the 3 `Species` (in rows). 
Save the resulting table of means as a tibble `im1` (2).

**`r nr`c.** Create a histogram that shows the distribution of `Sepal.Width` values 
across all species (1). 

**`r nr`d.** Create a graph that shows the shape of the distribution of `Sepal.Width` values 
separately for each species (1). 

**`r nr`e.** Create a graph that shows `Petal.Width` as a function of `Sepal.Width` 
separately for each species (1). 

```{r, iris_EDA}
# ?iris

## (a) Turn into tibble and inspect: ----- 
ir <- as_tibble(datasets::iris)
dim(ir)        # => 150 observations (rows) x 5 variables (columns)
sum(is.na(ir)) # =>   0 missing values

## (b) Compute counts and means by species: ----- 
im1 <- ir %>%
  group_by(Species) %>%
  summarise(n = n(),
            mn_sep.len = mean(Sepal.Length),
            mn_sep.wid = mean(Sepal.Width),
            mn_pet.len = mean(Petal.Length),
            mn_pet.wid = mean(Petal.Width)
            )

# Print im1 (as table with 4 variables of means): 
knitr::kable(im1, caption = "Average iris measures (4 variables of mean values).") 

## Graphical exploration: ----- 

## Distribution of 1 (continuous) variable:

## (c) Distribution of Sepal.Width across species:
ggplot(ir, aes(x = Sepal.Width)) +
  geom_histogram(binwidth = .1, fill = "forestgreen") + 
  labs(title = "Distribution of sepal width across iris species") +
  theme_bw()

## Distributions/relationships between 2 variables (1 categorical, 1 continuous):

## (d) The distributions of Sepal.Width by species:
ggplot(ir, aes(x = Species, y = Sepal.Width, fill = Species, shape = Species)) +
  geom_violin() +
  geom_point(size = 4, alpha = 1/2, position = "jitter") +
  labs(title = "Distributions of sepal width by iris species") +
  theme_bw()

## Alternative solution using density plots: 
ggplot(ir, aes(x = Sepal.Width, fill = Species)) +
  facet_wrap(~Species) + 
  geom_density() +
  labs(title = "Distributions of sepal width by iris species") + 
  coord_fixed() +
  theme_bw()

## Relationships between 2 variables (2 continuous):

## (e) Petal.Width as a function of Sepal.Width by iris species: 
ggplot(ir, aes(x = Sepal.Width, y = Petal.Width, color = Species, shape = Species)) +
  facet_wrap(~Species) +
  geom_jitter(size = 3, alpha = 2/3) +
  # geom_density2d() +
  coord_fixed() +
  labs(title = "Petal width as a function of sepal width by iris species") +
  theme_bw()

## next: Turn iris df into long format using tidyr::gather ----- 
## (see below)
```

### Tidy data

**`r nr`f.** Re-format your tibble `ir` into a tibble `ir_long` which in "long format" (2). 

Hint: Use `tidyr::gather` and `separate` to turn `ir` into a tibble that contains only 1 dependent variable for the value of measurements (e.g., `val`), but 2 categorical variables that specify the `part` (Sepal vs. Petal) and `metric` (Length vs. Width) of each observation.

**`r nr`g.** Use `ir_long` to recompute the subgroup means (for each combination of species, plant part, and metric) computed in **`r nr`b.**.  Save the resulting table of means as a tibble `im2` and verify that they have not changed from `im1` above (2). 

**`r nr`h.** Visualize the relationships between the means of `im2` (i.e., the mean measurements by plant part and metric) separately for each species (2). 

Hints: This task asks for showing the value of a continuous variable (the value of means) as a function of 2 categorical variables (the plant `part` and type of `metric`). Possible solutions could incorporate either `geom_line` or `geom_tile` and use different facets for different `Species`. 

**`r nr`i.** **Bonus task:** Re-format your tibble `ir_long` (in long format) into a wider tibble `ir_short` that corresponds to the original `ir` dataset (2). 

Hint: This task primarily calls for applying `tidyr::spread` and `unite` commands on `ir_long`. 
However, `spread` will encounter an error unless every individual plant is identified by a unique variable (e.g., `id` number).  This can be achieved by adding a numeric counter variable `id` (with values of `rep(1:50, 3)`) to `ir` _before_ creating `df_long`. 

```{r, iris_tidyr}
## Data (from above): 
# ?iris
# ir <- as_tibble(datasets::iris)
# ir

## (f) Re-format ir into long format (using tidyr::gather) ----- 
ir_long <- ir %>% 
  mutate(id = rep(1:50, 3)) %>%   # add a unique id to each plant [to enable (i) below]
  gather(Sepal.Length:Petal.Width, key = "type", value = "val") %>%
  separate("type", into = c("part", "metric"), sep = "\\.")

ir_long
dim(ir_long) # => 600 rows x 5 columns

## (g) Recompute group counts and means from ir_long: ----- 
im2 <- ir_long %>%
  group_by(Species, part, metric) %>%
  summarise(n = n(),
            mn_val = mean(val)
            )

# Print im2 (as table with 4 variables of means): 
knitr::kable(im2, caption = "Average iris measures (1 variable of mean values).")

## Check: Compare sums of all means in im1 vs. im2: 
sum1 <- sum(im1$mn_sep.len) + sum(im1$mn_sep.wid) + 
        sum(im1$mn_pet.len) + sum(im1$mn_pet.wid)  # => 41.574
sum2 <- sum(im2$mn_val)  # => 41.574
sum1 == sum2             # => TRUE (qed)


## Showing the value of a continuous variable by 2 categorical variables: ----- 

## (h) Visualize the means of im2 (i.e., mean measurements by plant part and metric) 
##     as a line plot separately for each species:
ggplot(im2, aes(x = part, y = mn_val, group = metric, color = metric)) +
  facet_wrap(~Species) +
  geom_point(aes(shape = metric), size = 3) +
  geom_line(aes(linetype = metric)) +
  labs(title = "Mean petal and sepal lengths and widths by iris species") + 
  theme_bw()

## Alternative solution using tile plots:
ggplot(im2, aes(x = part, y = metric)) +
  facet_wrap(~Species) +
  geom_tile(aes(fill = mn_val)) +
  geom_text(aes(label = mn_val), color = "white") + 
  labs(title = "Mean petal and sepal lengths and widths by iris species") + 
  theme_bw()

## Note: Using raw values (df_long) to visualize the distributions of val 
##       as a function of Species, plant part, and metric:
ggplot(ir_long, aes(x = part, y = val, fill = Species)) + 
  facet_wrap(~Species) +
  geom_violin() +
  geom_jitter(aes(shape = metric), size = 2, alpha = 2/3) + 
  labs(title = "Mean petal and sepal lengths and widths by iris species") + 
  theme_bw()

## (i) Turn df_long back into original wide format using tidyr::spread -----
# ir_long

# PROBLEM with original iris data:  
# Many val have identical combinations of values of 
# Species, part, and metric. 

# This is due to the original iris dataset, which include 50 plants
# for each species that were not further distinguished.

# However, this creates a problem when trying to re-format from 
# long format into a wider format. Doing so would create an error
# "Error: Duplicate identifiers ..." 

# Thus, we need to add a variable `id` that uniquely identifies each plant. 

# Hack: Adding id to ir_long to recover original id to each individual plant.  
#       (Note that this relies on knowing the order of plants in ir_long):

# ir_long$id <- rep(1:50, 4*3)
# ir_long

# A better solution: Adding id = 1:50 to the _original_ data set 
# (before turning it into long format above)!                             

## (i) Re-format ir_long into shorter format (using tidyr::spread) ----- 
ir_short <- ir_long %>%
  unite(type, part, metric, sep = ".") %>%  # unite part and metric into "type" column
  spread(key = type, value = val) %>%       # spread "type" variable into multiple columns
  arrange(Species, id)

ir_short

## Verify identity of all measurement values in ir and ir_short: 
all.equal(ir$Sepal.Length, ir_short$Sepal.Length) & 
all.equal(ir$Sepal.Width,  ir_short$Sepal.Width)  &
all.equal(ir$Petal.Length, ir_short$Petal.Length) & 
all.equal(ir$Petal.Width,  ir_short$Petal.Width)  # => TRUE (qed) 
```

+++ here now +++


## Own datasets

### Create people data

Create a (psychological) dataset that allows illustrating contents from all chapters. 

```{r, create_people_data, include = TRUE}
n <- 1000     # [n]umber of participants
set.seed(42)  # for replicability

## Demographics: -----

## Generate random initials: ----
r_initials <- function(n) {

  stopifnot(is.numeric(n), n > 0) # check conditions
  
  initials <- rep("N.N", n) # initialize output vector
  
  for (i in 1:n) {
    initials[i] <- paste0(paste(sample(LETTERS, 1), sample(LETTERS, 1), sep = "."), ".")
  }
  return(initials)
}

## Check:
# r_initials(100)
# length(LETTERS)^2 # => 676 possible sequences
# length(unique(r_initials(10000))) # => 676 
initials <- r_initials(n)

## Sex/gender: 
sex <- sample(x = c(0, 1), size = n, prob = c(.54, .46), replace = TRUE)
sex <- factor(sex, labels = c("female", "male"))

## Generate a (pseudo) random age distribution: ---- 
r_ages <- function(n, min = 16, max = 92) {
  
  stopifnot(is.numeric(n), n > 0) # check conditions

  ages <- rep(NA, n) # initialize output vector
  
  # (a) sample from stepwise distribution: 
  ages <- sample(size = n, 
                 x = c(min:max, 18:85, 20:39, 22:36, 24:33, 26:30, 35:63, 44:61, 65:77), # oversampling some regions
                 replace = TRUE)
  
  # (b) smoothing by adding random noise: 
  ix.not_extreme <- which((ages > (min + 2)) & (ages < (max - 2)))
  ages[ix.not_extreme] <- ages[ix.not_extreme] + sample(size = length(ix.not_extreme), -2:2, replace = TRUE)
  
  return(ages)
}

ages <- r_ages(n)

## Check:
ggplot(as_tibble(ages), aes(x = value)) +
  geom_histogram(binwidth = 1, fill = seeblau) # +
  # geom_density()

## Big sample: 
# ggplot(as_tibble(r_ages(100000)), aes(x = value)) +
#  # geom_density() + 
#  geom_histogram(binwidth = 1, fill = seeblau)


## Convert ages into birthdays: ----
library(lubridate)

r_bday <- function(ages) {
  
  stopifnot(is.numeric(ages), ages > 0) # check conditions

  n <- length(ages)
  bdays <- rep(NA, n) # initialize output 
  
  # Compute bdays as today() - age (in years) - some random day within this year:
  bdays <- today() - years(ages) - days(sample(size = n, 0:365, replace = TRUE)) 
  
  return(bdays)
  
}

bdates <- r_bday(ages)
byears <- year(bdates)
bmonths <- month(bdates, label = FALSE)
bdays <- day(bdates) 
bmdays <- paste0(bmonths, "/", bdays)
bweekdays <- wday(bdates, label = TRUE)

## Check:
t <- tibble(id = initials,
            sex = sex, 
            age = ages,
            bdate = bdates,
            byear = year(bdates),
            bmonth = month(bdates, label = FALSE),
            bday = day(bdates), 
            bmday = paste0(bmonths, "/", bdays), 
            bweekday = wday(bdates, label = TRUE), 
            age_y = year(today())- year(bdates)  # Note deviation from ages 
            # if bday hasn't been reached yet this year
)
# t

# t %>% arrange(bdate)
# t %>% arrange(age)

## Height: ----- 
## From https://en.wikipedia.org/wiki/List_of_average_human_height_worldwide 

##          Average male:             Average female:
## -----------------------------------------------------##
## Germany: 178 cm (5 ft 10 in) 	    165 cm (5 ft 5 in)
## USA:     175.7 cm (5 ft 9 in) 	    161.8 cm (5 ft 3 1⁄2 in)

heights <- rep(NA, n)

noise_female <- round(rnorm(sum(sex == "female"), mean = 0, sd = 8), 0)
noise_male   <- round(rnorm(sum(sex == "male"), mean = 0, sd = 11), 0)

heights[sex == "female"] <- 165 + noise_female
heights[sex == "male"]   <- 178 + noise_male

## Reduce height by age (1cm per decade): 
heights[byears < 2000] <- heights[byears < 2000] -  1
heights[byears < 1990] <- heights[byears < 1990] -  1
heights[byears < 1980] <- heights[byears < 1980] -  1
heights[byears < 1970] <- heights[byears < 1970] -  2
heights[byears < 1960] <- heights[byears < 1960] -  1
heights[byears < 1950] <- heights[byears < 1950] -  2
heights[byears < 1940] <- heights[byears < 1940] -  1
heights[byears < 1930] <- heights[byears < 1930] -  2

## Convert metric (cm) into imperial (feet and inches)...

## Check:
sex_height <- tibble(sex = sex,
                     byear = byears, 
                     height = heights)

## Height by sex:
ggplot(as_tibble(sex_height), aes(x = sex, y = height, color = sex)) +
  geom_violin() +
  geom_jitter(alpha = 1/2)

## Height by age and sex:
ggplot(as_tibble(sex_height), aes(x = byears, y = height)) +
  facet_wrap(~sex) +
  geom_jitter(alpha = 1/3) +
  geom_smooth()


## Blood type: -----
## From https://en.wikipedia.org/wiki/Blood_type_distribution_by_country 

## Types:   O+ 	   A+ 	    B+ 	    AB+ 	  O− 	    A− 	    B− 	    AB−
## --------------------------------------------------------------------- ##
## Germany:35.0% 	 37.0% 	  9.0% 	  4.0% 	  6.0% 	  6.0% 	  2.0% 	  1.0%
## USA:    37.4% 	 35.7% 	  8.5% 	  3.4% 	  6.6% 	  6.3% 	  1.5% 	  0.6%
## World:  38.67%  27.42%  22.02% 	5.88% 	2.55% 	1.99% 	1.11% 	0.36%

blood_types <- c("O+", "A+", "B+", "AB+", "O−", "A−", "B−", "AB−")
# blood_probs <- c(.35, .37, .09, .04, 	.06, .06,  .02,	.01)  # Germany
blood_probs <- c(.374, .357, .085, .034, 	.066, .063,  .015,	.006)  # USA
# sum(blood_probs) # should be 1.00

btypes <- sample(size = n, blood_types, prob = blood_probs, replace = TRUE)
# table(btypes)

## Other possible IVs: ----- 
## - education (categorical)
## - weight (compute via height and BMI distribution)

## Combine IVs: ----- 
IVs <- tibble(name = initials,
              gender = sex, 
              # bdate = bdates,
              byear = byears, 
              bmonth = bmonths,
              bday = bdays,
              bweekday = bweekdays,
              height = heights, 
              blood_type = btypes)
IVs


## DVs: -----

## - disposable income (numeric)
## - numeracy (categorical)
## - health status (numeric, e.g., 1:10 scale)
## - mood (1:5, on 3 days: Mon, Wed, Fri, and 2 times: 10:00 vs. 18:00)

## To compute (from IVs):
## - Age (from byear, but adjusting by bday)
## - Birth season (spring, summer, autumn, winter)
## - Zodiac sign (from birthday, using cut() or join functions)
```


### Create experimental data

```{r, create_exp_data, include = TRUE}
n <- 10       # [n]umber of participants
set.seed(88)  # for replicability

IVs <- data.frame("name" = c("Ann", "Bea", "Cat", "Deb", "Eva", "Fred", "Gary", "Hans", "Ian", "John"),
                  "gender" = c(rep("f", 5), rep("m", 5)), 
                  "age" = sample(18:65, n, replace = TRUE)
                   )
IVs

## (a) within-subjects conditions (with multiple tasks per person):
DVs <- data.frame("task.1" = rep(c("red", "blue"), 5),
                   # "pos.1" = rep(1, n),
                   "time.1" = sample(10:99, n), 
                   "task.2" = rep(c("blue", "red"), 5),
                   # "pos.2" = rep(2, n),
                   "time.2" = sample(10:99, n)
                   )
DVs


## (b) between-subjects conditions (with separate variables):

# DVs2 <- data.frame("cond" = c(rep("A", n/2), rep("B", n/2)),
#                    "A.num1" = c(sample(1:7, n/2), rep(NA, n/2)), 
#                    "B.num1" = c(rep(NA, n/2), sample(3:9, n/2)),
#                    "A.chr1" = c(sample(c("ABBA", "Beatles"), n/2, replace = TRUE), rep(NA, n/2)), 
#                    "B.chr1" = c(rep(NA, n/2), sample(c("ABBA", "Pink Floyd"), n/2, replace = TRUE))
#                    )

DVs2 <- data.frame("cond" = rep(c("A", "B"), n/2))

DVs2$A.num1 <- NA
DVs2$A.num1[DVs2$cond == "A"] <- c(sample(1:6, n/2, replace = TRUE))
DVs2$B.num1 <- NA
DVs2$B.num1[DVs2$cond == "B"] <- c(sample(4:9, n/2, replace = TRUE))

DVs2$A.chr1 <- NA
DVs2$A.chr1[DVs2$cond == "A"] <- c(sample(c("Abba", "Beatles"), n/2, replace = TRUE))
DVs2$B.chr1 <- NA
DVs2$B.chr1[DVs2$cond == "B"] <- c(sample(c("Beatles", "Zappa"), n/2, replace = TRUE))
DVs2

# Note that DVs encodes order (or chronological trial position) 
# implicitly (as 1st vs. 2nd entry) for every case.

## Combine IVs and DVs: 
exp <- cbind(IVs, DVs)
exp2 <- cbind(IVs, DVs2)

# exp
# exp2

dim(exp)
dim(exp2)
```

Basic manipulations of `exp` data (in base R):

```{r, manipulate_exp_data, include = TRUE}
exp.t <- exp # temporal copy

# 1. Adding 2 new variables/columns to a df by assigning/using it: 
exp.t$id <- 1:nrow(exp.t)
exp.t$bnt <- sample(1:4, n, replace = TRUE)
# exp.t

# 2. Swap some columns (e.g., putting id to front): 
id.col  <- which(names(exp.t) == "id")  # determine id column
bnt.col <- which(names(exp.t) == "bnt") # determine bnt column
exp.t <- exp.t[ , c(id.col, 1:(id.col - 1), bnt.col)]
exp.t

# 3. Sorting cases: 
exp.t[order(exp.t$task.1, exp.t$bnt),] # sort cases by task.1 and bnt values

# 4. Reversing cases and variables: 
#    Reversing the (arbitrary) order of all cases and all variables in df:
exp.t[rev(1:nrow(exp.t)), rev(1:ncol(exp.t))]
```

Same steps in the `tidyverse`:

```{r, manipulate_exp_data_tidyverse, include = TRUE}
exp.t2 <- exp # temporal copy

# 1. Adding 2 new variables/columns to a df by assigning/using it: 
exp.t2 <- exp.t2 %>%
  mutate(id = 1:nrow(exp.t2),
         bnt = sample(1:4, n, replace = TRUE))
# exp.t2

# 2. Swap some columns (e.g., putting id to front): 
exp.t2 <- exp.t2 %>%
  select(id, everything())
# exp.t2

# 3. Sorting cases:
exp.t2 <- exp.t2 %>%
  arrange(task.1, bnt)
# exp.t2

# 4. Reversing cases and variables: 
#    Reversing the (arbitrary) order of all cases and all variables in df:
exp.t2 %>%
  mutate(row = 1:nrow(exp.t2)) %>%  # add helper column 
  arrange(desc(row)) %>%
  select(-row) %>%                  # remove helper column again 
  select(rev(names(exp.t2)))
```

[This file last updated on `r Sys.time()` by [hn](http://neth.de/).]

<!-- eof. --> 